# Optimization with NTK

Le contenu de cette présentation peut être trouvé dans :

[1] [Handbook of Convergence Theorems for (Stochastic) Gradient Methods](http://arxiv.org/abs/2301.11235)

[2] [Mes notes de cours](https://cloud.math.univ-paris-diderot.fr/s/XZkLDz8ErQnjtKF)

[3] [Loss landscapes and optimization in over-parameterized non-linear systems and neural networks [Liu et al - 2022]](https://arxiv.org/abs/2003.00307)

Je précise en dessous quelles références j'ai utilisées au long de l'exposé.

## Sommaire de l'exposé

1. Optimisation stochastique
   1. Gradient Descent [1, Section 3.1 & 3.2]
   2. Stochastic Gradient Descent [1, Sections 5.1 & 5.2]
   3. Interpolation et Variance [1, Section 4.3]
   4. Nonconvex with Polyak-Łojasiewicz [1, Sections 2.4 & 3.3 & 5.3]
2. Large Neural Networks and optimization [2, Chap 4, II.3]
   1. Propriétés des NNs [3, Theorem 4]
   2. Optimization des NNs avec PL [3, Theorems 6 & 7]