# Les algorithmes stochastiques peuvent converger vers des lois à queue  lourde 

**Résumé :** durant les deux dernières séances, nous avons vu des résultats  de quasi-convergence pour des algorithmes d'optimisation de type SGD sur  des fonctions fortement convexes, PL, ou des réseaux de neurones dans le  régime NTK. Je vais montrer que dans certains cas, les itérées de  l'algorithme SGD convergent vers une distribution à queue lourde. On  utilisera pour cela des théorèmes célèbres de Kesten sur les récurrences  linéaires aléatoires. Ce phénomène heavy-tail est observable  empiriquement sur la dynamique d'entraînement des réseaux de neurones,  et montre qu'il n'est pas toujours pertinent de comparer cette dynamique  à des processus stochastiques drivés par un mouvement brownien, mais que  les processus drivés par des Lévy sont peut-être un meilleur choix. Je  m'appuierai sur l'article The Heavy-Tail phenomenon in SGD de  Gürbüzbalan, Şimşekli et Zhu.
